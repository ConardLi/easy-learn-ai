#### **模型与能力**  
##### **Moonshot 发布 Kimi K2.5：开源多模态 MoE 新 SOTA**  
Kimi K2.5 继续用 32B 激活 /1T 参数 MoE 架构，在 HLE、BrowseComp、MMMU Pro、VideoMMMU、SWE-bench 等代理、视觉和代码基准上拿到开源模型第一。模型原生支持图像+视频理解，可从屏幕录屏还原网页，支持 128K→256K 上下文、INT4 部分量化，并已在 HuggingFace、Ollama、Together、Fireworks 等平台上线，部分场景可在本地多卡 Mac 上跑起来。  
 > 相关链接：[官方技术博客](https://www.kimi.com/blog/kimi-k2-5.html)｜[Moonshot 发布线程](https://twitter.com/Kimi_Moonshot/status/2016065333694771276)｜[Zach Mueller 技术拆解](https://twitter.com/TheZachMueller/status/2016183468430860587)｜[LMArena 开源模型榜单](https://twitter.com/arena/status/2016294722445443470)｜[本地运行示例（M3 Ultra）](https://twitter.com/awnihannun/status/2016221496084205965)  

##### **Arcee / Prime Intellect 发布 Trinity Large：400B MoE 西方开源反击**  
Arcee 联合 Prime Intellect、Datology 推出 Trinity Large 预览版：400B MoE、13B 激活，训练 17T token，3:1 局部/全局 gated attention、SWA、NoPE+RoPE、深度缩放归一化、Muon 优化器等，约 2000 块 B300 连训 1 个月。vLLM 提供首日推理支持，OpenRouter 暂时免费开放。  
 > 相关链接：[Arcee 公告](https://twitter.com/arcee_ai/status/2016278017572495505)｜[Prime Intellect 介绍](https://twitter.com/PrimeIntellect/status/2016280792037785624)｜[技术要点汇总](https://twitter.com/samsja19/status/2016283855888773277)｜[vLLM 支持](https://twitter.com/vllm_project/status/2016322567364346331)  

##### **DeepSeek-OCR 2：学会“阅读顺序”的文档 OCR**  
DeepSeek-OCR 2 在 HuggingFace 上开源，引入 Visual Causal Flow 和 DeepEncoder V2，可将图片压缩到约 256–1120 视觉 token，OmniDocBench v1.5 得分 91.09%（+3.73）。社区评测认为在真实 SOTA 之下但路线值得关注，vLLM 已支持推理。  
 > 相关链接：[模型主页](https://huggingface.co/deepseek-ai/DeepSeek-OCR-2)｜[vLLM 支持](https://twitter.com/vllm_project/status/2016065526058090967)｜[Jerry Liu 解读](https://twitter.com/jerryjliu0/status/2016319238974407146)｜[社区评价](https://twitter.com/teortaxesTex/status/2016179572056678739)  

##### **OpenAI Prism：GPT‑5.2 驱动的“科研版 Overleaf”**  
OpenAI 推出免费科研工作区 Prism，整合 LaTeX 写作、协作和文献管理，由 GPT‑5.2 提供改写、引用、检索等能力，对所有 ChatGPT 个人账号开放。官方强调数据使用遵循 ChatGPT 现有策略，不会自动攫取科研成果 IP。  
 > 相关链接：[OpenAI 发布](https://twitter.com/OpenAI/status/2016209462621831448)｜[产品负责人说明](https://twitter.com/kevinweil/status/2016210486778642808)｜[数据/IP 说明](https://twitter.com/kevinweil/status/2016285175106420867)  

##### **Qwen 系列：思考版与量化能力进展**  
阿里 Qwen3-Max-Thinking 宣称推理力接近商用闭源模型，但社区反馈其代码代理模式目前存在编译问题。Qwen3‑32B 在 H100 上做 INT4 量化测试，显示在 MMLU‑Pro 上仅损失约 1.9% 精度，却能将并发用户数从 4 提升到 47（4k 上下文），体现大模型在低比特部署上的潜力。  
 > 相关链接：[Qwen3-Max-Thinking 介绍](https://qwen.ai/blog?id=qwen3-max-thinking)｜[Qwen3‑32B 量化评测](https://research.aimultiple.com/llm-quantization/)  

##### **Transformers v5 正式版：MoE 加速与大规模提速**  
Hugging Face Transformers v5 发布，对 MoE 提示 6–11 倍性能提升，并简化 tokenizer/后端配置，动态权重加载更快，支持量化+MoE+并行+PEFT。社区实测：单请求推理快约 50%，并发推理吞吐翻倍。  
 > 相关链接：[Transformers v5 仓库与迁移指南](https://github.com/huggingface/transformers)  

 

---  


#### **Agent 与工具链**  
##### **Kimi Agent Swarm：最多 100 个子 Agent 并行协作**  
Kimi K2.5 内置 Agent Swarm（付费用户 Beta），可动态生成多达 100 个子 Agent，最多执行 1500 次工具调用，号称端到端任务耗时可缩短到单 Agent 的约 1/4–1/3。背后采用并行 Agent 强化学习（PARL）训练调度策略。  
 > 相关链接：[Agent Swarm 说明](https://twitter.com/eliebakouch/status/2016025747144483060)｜[技术报告摘要](https://twitter.com/TheZachMueller/status/2016183781481132443)  

##### **Kimi Code 与 Agent SDK：开源编码助手与私有 Agent 框架**  
Moonshot 同时发布 Kimi Code 开源编码 Agent（Apache‑2.0），支持常见 IDE/编辑器集成，并给出 Agent SDK 方便开发者构建自定义 Agent 工作流。官方帐号还开始集中发布提示词和场景案例。  
 > 相关链接：[Kimi Code 公告](https://twitter.com/Kimi_Moonshot/status/2016034259350520226)｜[Agent SDK 公告](https://twitter.com/Kimi_Moonshot/status/2016034272998809678)｜[Kimi Product 账号](https://twitter.com/Kimi_Moonshot/status/2016082808834531825)｜[视频转网页 Demo](https://twitter.com/KimiProduct/status/2016081756206846255)  

##### **LangChain、Jules 等推动“多子 Agent + 规划评论员”模式**  
Kimi Swarm、LangChain 子 Agent、Google Jules 的 Planning Critic 展现出共识架构：一个总控 Agent 动态拆任务给并行子 Agent，外加“规划审稿人”先批判再执行。Jules 声称通过计划评论器可降低约 9.5% 任务失败率。  
 > 相关链接：[LangChain 子 Agent 模式](https://twitter.com/sydneyrunkle/status/2016285836581765461)｜[Jules Planning Critic](https://twitter.com/julesagent/status/2016178107019837917)  

##### **IDE 侧 Agent：Cursor、VS Code MCP 等持续演进**  
Cursor 强调语义搜索索引大型代码库后，Agent 质量和响应速度显著提升；VS Code 正在增强“命令执行解释”等安全 UX，并通过 MCP Apps 让工具服务器直接返回 UI 组件（如智能灯控制面板）。  
 > 相关链接：[Cursor 语义搜索更新](https://twitter.com/cursor_ai/status/2016202243499073768)｜[VS Code Agent 体验更新](https://twitter.com/aerezk/status/2016225215802397146)｜[MCP Apps UI 示例](https://twitter.com/burkeholland/status/2016208751200457088)  

##### **多 Agent 编程助手实践：Claude “蜂巢” 与 Autogen/CheshireCat 等**  
社区有人基于 Claude Code 搭了 7 个专职 Agent（写代码、测试、评审等）共享 SQLite+FTS5 记忆的“蜂巢”，以 MCP 服务器形式接入，也被拿来对比微软 Autogen、BMAD 等多 Agent 框架。CheshireCat 企业版则主打多租户 Agent 工作流。  
 > 相关链接：[Claude 多 Agent 项目](https://github.com/blackms/aistack)｜[Microsoft Autogen](https://github.com/microsoft/autogen)｜[CheshireCat 核心仓库](https://www.github.com/matteocacciola/cheshirecat-core)  

##### **Karpathy 明确押注“Agent First 编程”**  
Andrej Karpathy 公开表示将工作流迁移到以 Claude 等 LLM Agent 为核心，让模型持续改代码、跑实验、人只做监督，认为“模型永不疲倦”是关键优势。这与 Manus、Cursor 等产品的 Agent 化路线高度一致。  
 > 相关链接：[Karpathy 相关帖子](https://xcancel.com/karpathy/status/2015883857489522876)  

 

---  


#### **基础设施与硬件**  
##### **Unsloth：MoE 训练提速 14×，目标 30×**  
Unsloth 宣布基于新内核和 Transformers v5，MoE 训练已经比 v4 快约 14 倍，并计划再翻倍达到 30×。同时支持最新 vLLM/Transformers 生态，主攻低成本在消费级/云 GPU 上训大 MoE 模型。  
 > 相关链接：[官方加速公告](https://x.com/UnslothAI/status/2015935368525447395)  

##### **FlagOS：试图打通“模型–系统–芯片”的统一栈**  
GPU MODE 社区提出 FlagOS，目标是做一个开源系统软件栈，把模型、系统、芯片三层打通，让 AI 负载在不同硬件之间更易迁移和调优，强调从 ML 系统、编译器到硬件共设计的经验沉淀。  
 > 相关链接：[FlagOS 讨论线程](https://discord.com/channels/1189498204333543425/1189498205101109300)  

##### **Tinygrad 与 Megakernel：从 FlashAttention 到“GPU 上的 OS”**  
tinygrad 社区已能从朴素 attention 前端自动重写生成 FlashAttention 内核，并探索把模型编译成大一体 Megakernel，减少调度和显存往返。George Hotz 引用 Luminal 文章，认为未来 GPU 更像跑一个内置“操作系统”的大内核。  
 > 相关链接：[Megakernel 博文](https://blog.luminal.com/p/compiling-models-to-megakernels)  

##### **多 GPU / 老显卡本地 LLM：带宽和散热是硬伤**  
Reddit 和 Eleuther 讨论用二手 Tesla 等堆到 200GB+ VRAM 跑大模型。实测瓶颈多在 PCIe 带宽和大 prompt 预填充速度而非 token/s，散热和功耗也不划算。很多 benchmark 还没覆盖“多卡切片大模型在线服务”这一真实场景。  
 > 相关链接：[本地多 GPU 基准与讨论](https://esologic.com/gpu-server-benchmark/#gpu-box-benchmark)  

##### **FlashInfer-Bench / MLSYS26：开源推理内核竞赛数据集放出**  
FlashInfer 团队在 HuggingFace 发布推理 trace 和专门为 MLSys 2026 大赛准备的工作负载数据集，并计划做双周排行榜，方便大家在统一数据上优化解码内核、MoE 内核等。  
 > 相关链接：[通用 trace 数据集](https://huggingface.co/datasets/flashinfer-ai/flashinfer-trace)｜[MLSys26 竞赛数据集](https://huggingface.co/datasets/flashinfer-ai/mlsys26-contest)  

 

---  


#### **研究与方法**  
##### **数学证明：LLM“永远会幻觉”，越越狱越严重**  
一篇在 BASI 讨论的论文从理论上证明，在当前范式下 LLM 不可能彻底杜绝幻觉，很多 jailbreak 技巧正是利用了这一点。社区指出，越狱会把模型语境拉偏，使其不再识别本应标记为恶意/不可靠的内容，从而进一步放大幻觉与安全风险。  
 > 相关链接：[论文：On the Inevitable Hallucinations of LLMs](https://arxiv.org/abs/2409.05746)  

##### **Anthropic 生物风险论文：微量微调即可“解锁”被压制能力**  
Anthropic 新论文表明，把开源模型在前沿闭源模型输出上做少量微调，就能恢复甚至增强本被安全训练压制的生物风险等危险能力，且算力需求不高。Eleuther 社区认为这证明“仅靠拒答”非常脆弱，开源模型的双重用途风险被低估。  
 > 相关链接：[Anthropic 论文 PDF](https://arxiv.org/pdf/2601.13528)｜[官方推文](https://x.com/AnthropicAI/status/2015870963792142563)  

##### **DeepPlanning 等长程规划与 RL 提效研究**  
DeepPlanning 提出可验算约束的长程任务基准（多日旅行、购物等），显示当前 Agent 在真实规划上仍吃力。PrefixRL 等工作尝试用“前缀条件”重用旧轨迹，让复杂推理 RL 收敛速度提升约 2 倍。  
 > 相关链接：[DeepPlanning 讨论](https://twitter.com/iScienceLuvr/status/2016122154862182792)｜[PrefixRL 讨论](https://twitter.com/iScienceLuvr/status/2016125085825040852)  

##### **多语种 scaling law 与 FrontierMath：离“解决数学研究问题”还很远**  
Google Research 的 ATLAS 给出多语种大模型在数据配比与模型大小上的 scaling law 指南；Epoch 的 FrontierMath: Open Problems 榜单开放给模型挑战，目前还没有任何 AI 解出收录的真·数学难题，用来校准对“AI 研究员”能力的预期。  
 > 相关链接：[ATLAS 多语种 scaling law](https://twitter.com/GoogleResearch/status/2016234343602258274)｜[FrontierMath 基准](https://twitter.com/EpochAIResearch/status/2016188014540816879)  

##### **MergeMix：用“可学习模型合并”调数据配比**  
MergeMix 提出在训练中期通过“可学习的模型合并”来自动搜索更优数据混合比例，面向预算有限的开源项目。社区认为对想在有限算力下挤出更多性能的团队很实用。  
 > 相关链接：[MergeMix 论文](https://arxiv.org/pdf/2601.17858)  

 

---  


#### **产品与应用落地**  
##### **Kimi K2.5 办公网 Agent：大体量“综述+写作”场景落地**  
K2.5 在国内用户中被大量用于报告撰写、资料汇总等长文档办公场景，Moonshot 专门做了“Office Productivity / K2.5 Agent”，强调从多文档检索到成稿的一站式自动化，部分公司已经用它替代传统周报/分析报告流程。  
 > 相关链接：[K2.5 办公 Agent 介绍图](https://www.kimi.com/blog/kimi-k2-5.html)  

##### **Gemini AI Studio 大幅降配：长上下文用户被迫找替代品**  
Google 下调了 Gemini AI Studio 免费额度，并被曝 Pro/Ultra 实际“热内存”仅 32k–128k token，远低于宣传的百万级，用户抱怨 Pro 体验甚至不如免费版。很多依赖长上下文工作流的用户开始转向 Grok 4.1（2M）和 Claude Sonnet 4.5（1M）等方案，或改用向量检索替代生吃大上下文。  
 > 相关链接：[上限缩水讨论帖](https://www.reddit.com/r/GeminiAI/comments/1qnvbjr/32768_or_215_tokens_in_hot_memory_gemini_has_been/)｜[AI Studio 限额公告讨论](https://www.reddit.com/r/Bard/comments/1qngot0/about_the_recent_ai_studio_limit_downgrade/)  

##### **Perplexity Pro 与 Kagi：用户开始用“钱包”投票**  
Perplexity Pro 用户频繁遇到搜索和图像生成被限流、计费不透明等问题，特别是印度地区支付失败，部分人考虑转向主打隐私和 Claude 接入的 Kagi 搜索。说明“类浏览器问答”赛道已经从模型能力拼到稳定性与计费体验。  
 > 相关链接：[Perplexity Discord 投诉串](https://discord.com/channels/1047197230748151888/1047649527299055688)  

##### **本地代码助手：Qwen Coder + Cline/LM Studio 的真实门槛**  
在 8GB VRAM + 32GB RAM 的大众配置下，大家实测 Qwen2.5/3 Coder 7B/30B 是可用上限，但结合 Cline 等多 Agent 插件时很容易遇到 CUDA 内存错误，需要手动降上下文长度和仔细调参数，本地“Copilot 级体验”离即插即用还有距离。  
 > 相关链接：[LM Studio 硬件与模型讨论](https://discord.com/channels/1110598183144399058/1110598183144399061)  

 

---  


#### **行业与公司动态**  
##### **中美开源大模型“反向碾压”：Kimi K2.5 vs 西方闭源**  
Kimi K2.5 在多项代理、代码和多模态基准上接近甚至超越 Claude Opus 4.5、Gemini 3 Pro 等闭源模型，且以开源权重形式发布。部分分析认为，中国开源/廉价模型在企业侧已经对美国封闭模型形成明显性价比优势，a16z 也称 80% 初创在用中国产开源模型。  
 > 相关链接：[Artificial Analysis 榜单解读](https://twitter.com/ArtificialAnlys/status/2016250137115557953)｜[Reddit 投资视角长文](https://www.reddit.com/r/DeepSeek/comments/1qne7vt/enterpriseready_open_sourcechinese_ais_are_poised/)  

##### **MiniMax M2.2、Qwen 新型号等：春节前中资实验室密集“放烟雾”**  
MiniMax 预告 M2.2，“M2.1 已够强，M2.2 再升级”，社区期待其与 GLM 5 搭配在本地编码上的表现；Qwen 官方也在 ComfyUI PR 和 Twitter 通过 Z-Image 等名称暗示新一波视觉/多模态模型即将上线，典型“过年压轴发布”节奏。  
 > 相关链接：[MiniMax M2.2 预告](https://www.reddit.com/r/LocalLLaMA/comments/1qnfegx/minimax_is_teasing_m22/)｜[Qwen 新模型预热](https://www.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/)  

##### **Trinity Large 等“美国回归”：从微调回到从零预训练**  
Trinity Large 的出现被圈内视作“美国这边终于又有人从头训大模型，而不只是玩后处理和评测”。借助 Datology 提供的大规模精调数据和 Prime Intellect 自建算力，这条线如果跑通，会给西方开源注入新鲜血液。  
 > 相关链接：[Trinity Large 发布讨论](https://twitter.com/pratyushmaini/status/2016287361274138821)  

##### **Clawdbot 更名 Moltbot：安全与商标双重压力**  
热门编码 Agent Clawdbot 因与 Anthropic“Claude”商标冲突被迫更名为 Moltbot，更严重的是社区曝出其可无授权读取环境变量等敏感信息，被质疑为“零权限爬所有密钥”的安全雷区，多个社区开始明确劝退使用。  
 > 相关链接：[更名公告](https://xcancel.com/moltbot/status/2016058924403753024)｜[安全问题视频](https://youtu.be/7fltOAg8ZGI)  

##### **Decart 发布 Lucy 2，招人做实时视频模型内核优化**  
Decart 发布自回归视频编辑模型 Lucy 2，并在 GPU MODE 频道招工程师，重点是为实时视频/世界模型写低延迟 kernel，在 Trainium 3 等新加速器上跑。工作内容和传统 LLM 推理不同，偏视频和世界建模。  
 > 相关链接：[Lucy 2 技术报告](https://x.com/DecartAI/status/2016134190509498740)  

 

---  


#### **政策、治理与安全**  
##### **AI 检测工具“乱杀真人论文”：学术场景误伤严重**  
OpenAI Discord 等多处反馈，现有 AI 文本检测器把 GPT 出现前的论文也判成“AI 生成”，准确率极差，但高校和招聘仍在用。对研究者和学生来说，等于多了一层随机噪声审核。  
 > 相关链接：[OpenAI 服务器讨论](https://discord.com/channels/974519864045756446/998381918976479273)  

##### **Gemini Pro 实际上下文缩水被质疑“消费欺诈”**  
用户通过实验发现 Gemini Pro 热上下文实际只有 32k 左右，企业版也远低于宣称的 1–2M，且性能被认为倒退到 GPT‑3 级别，引发“虚标参数”的指控。再叠加限额下调和计费 bug（有人被错误扣费超 7 万美元），Google 在开发者口碑上受损。  
 > 相关链接：[上下文缩水爆料](https://www.reddit.com/r/GeminiAI/comments/1qnvbjr/32768_or_215_tokens_in_hot_memory_gemini_has_been/)｜[计费问题讨论](https://www.reddit.com/r/Bard/comments/1qngot0/about_the_recent_ai_studio_limit_downgrade/)  

##### **GPT‑5 “控制壳”泄露：更重的前置规则锁定**  
BASI Jailbreaking 有人贴出名为 GPT5_Hotfix.md 的文件，宣称是 GPT‑5 的“预生成控制壳”，在正式生成前先做语法约束、意图锁定和漂移防护。若属实，说明前沿闭源厂越来越依赖外围规则层来弥补模型本体问题。  
 > 相关链接：[控制壳文件截图](https://cdn.discordapp.com/attachments/1228043845967544380/1465691526947405888/GPT5_Hotfix.md)  

 

---  

  

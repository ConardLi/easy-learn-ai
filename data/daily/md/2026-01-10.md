#### **模型与能力**  
##### **DeepSeek 发布 MHC 训练方法，V4 传闻主打编码能力**  
DeepSeek 在论文中提出 Manifold‑Constrained Hyper‑Connections（MHC），用约束矩阵方式解决 27B 超大连接模型训练时的信号放大和梯度爆炸问题，被视为稳定深层/高连接结构的小但实用改进。同时多方消息称 DeepSeek V4 预计 2 月发布，主打长代码理解与生成，内部基准号称在编码上优于 GPT/Claude，但目前尚未正式上线。  
 > 相关链接：[MHC 技术解析与代码模拟](https://parthsharmaai.substack.com/p/deepseek-mhc-how-a-1967-algorithm)｜[V4 编码能力报道（The Information）](https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability)｜[V4 预计 2 月发布（Reuters）](https://www.reuters.com/technology/deepseek-launch-new-ai-model-focused-coding-february-information-reports-2026-01-09/Okwill)  

##### **Falcon‑H1R‑7B：小体量推理模型加入开放博弈**  
阿联酋 TII 的 Falcon‑H1R‑7B 作为 7B 级开源“推理向”模型被详细测评，在 Humanity’s Last Exam、τ²‑Bench Telecom 和 IFBench 上表现亮眼。缺点是许可证要求署名，影响“开放度”评分。整体被视为小模型推理赛道中的有力选手，继续给闭源大模型施压。  
 > 相关链接：[性能与开放度分析](https://twitter.com/ArtificialAnlys/status/2009690138604122238)｜[开放模型竞争趋势讨论](https://twitter.com/ArtificialAnlys/status/2009759874461081957)  

##### **FineTranslations：用 Gemma3 翻出 1 万亿 Token 平行语料**  
研究者用 Gemma3‑27B 把 FineWeb2 多语数据统一翻成英文，构建超 1T token 的平行语料 FineTranslations。适合做多语对齐、蒸馏、翻译/RAG 训练和评测，对做多语模型或跨语检索的人是一个现成的大型公共数据源。  
 > 相关链接：[数据集介绍](https://twitter.com/gui_penedo/status/2009677127671492616)  

##### **LTX‑2：可在 8GB 显存内跑的开源音视频生成模型**  
LTX‑2 是开源音频+视频生成模型，可在不到 8GB 显存上生成最长约 20 秒视频，在 4090 级显卡上生成 20 秒大约需 5 分钟，并提供 LoRA 训练代码。目前在开源 A/V 生成里算是比较实用的一档，适合想在本地做短视频/音视频 LoRA 的人尝试。  
 > 相关链接：[模型主页](https://ltx.io/model)｜[社区讨论 1](https://www.reddit.com/r/StableDiffusion/comments/1q8590s/thx_to_kijai_ltx2_ggufs_are_now_up_even_q6_is/)｜[社区讨论 2](https://discord.com/channels/714501525455634453/853983317044756510/1458914810686341303)  

##### **LFM 2.5 小模型在合成数据质量上挑战大模型**  
社区实测表明，LFM 2.5B 在生成训练用合成数据时，效果能接近甚至对标 Qwen3 30B Q8、Qwen3 235B Q3_XL 等大模型。一些人用 LFM2.5‑1.2B‑Instruct‑SDG 配合轻量前端做本地合成数据流水线，在约 1GB 显存即可跑通。  
 > 相关链接：[LFM2.5‑1.2B‑Instruct‑SDG](https://huggingface.co/MadlabOSS/LFM2.5-1.2B-Instruct-SDG)  

 

---  


#### **Agent 与工具链**  
##### **MCP 生态加速：官方服务器、mcp‑cli 和实现者涌入**  
OpenAI 阵营推出官方 MCP server，将文档、指南、AppsSDK 等统一成可直接被 IDE/代理消费的工具平面；社区又做了 mcp‑cli，用“发现”替代大段工具描述，号称能把 token 开销砍掉 99%。同时新开发者已开始按规范实现 MCP，并在 GitHub 提 issue，协议从“看文档”阶段进入“真实现”阶段。  
 > 相关链接：[OpenAI 方向 MCP server 介绍](https://twitter.com/reach_vb/status/2009686112986337309)｜[mcp-cli 细节](https://twitter.com/_philschmid/status/2009625698361573521)｜[MCP 规范实现疑问 issue](https://github.com/modelcontextprotocol/modelcontextprotocol/issues/2064)  

##### **Anthropic Skill.md + 各家“技能”系统，统一的 Agent 能力封装形态成型**  
Anthropic 在博客中用 skill.md 定义“技能”：一份 Markdown 描述+脚本/数据文件，Agent 按需读取，避免把所有工具说明塞进提示词。GitHub Copilot、Claude Code、Cline 等也都在推“skills/Agent Skills”，把复杂工作流拆成可版本化、可按需加载的指令包，成为 Agent 开发的通用抽象层。  
 > 相关链接：[Anthropic 技能设计](https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)｜[Claude Code 技能说明](https://twitter.com/claude_code/status/2009479585172242739)｜[VS Code Agent Skills 发布](https://twitter.com/code/status/2009744142335656156)｜[Cline 技能兼容更新](https://twitter.com/cline/status/2009793063753757024)  

##### **AI21 用 git worktree 做“事务型工作空间”，解决多 Agent 并发写文件**  
AI21 发现多子 Agent 同时写文件时，MCP 现有模型很快乱套，于是做了 MCP Workspace：提供 init/clone/compare/merge/delete 等原语，每个并发尝试用 git worktree 建独立工作目录，可同时跑到 16 个方案，最后合并赢家。这是把“事务 + 并发控制”真正落到代码工作区的一次实践。  
 > 相关链接：[AI21 工作空间方案线程](https://twitter.com/AI21Labs/status/2009565879600923100)  

##### **InfiAgent 与文件型工作区：长程 Agent 的核心变成“上下文工程”**  
InfiAgent 提出把长期状态都放进文件/目录，每一步只重建一个快照+固定的最近交互窗口，避免上下文无限膨胀。配合社区对“Agent 漂移”的讨论（何时开始跑题、行为不一致），大家越来越把“如何管理文件和上下文”视为做长程 Agent 的真正难点，而不是模型本身。  
 > 相关链接：[InfiAgent 思路总结](https://twitter.com/omarsar0/status/2009662975024447511)｜[Agent 漂移与稳定性指标讨论](https://twitter.com/dair_ai/status/2009657177989091423)  

##### **OpenRouter 路由升级：支持“性能底线”和技能加载**  
OpenRouter 新增“partition”排序，可以给自动路由设置性能底线，在不增加额外延迟的前提下只选足够快的模型；还上线 Provider Explorer 看各云商的模型覆盖。SDK 新增 Skills Loader，方便把 Anthropic 式 skill 包装成任何模型都能用的上下文片段。  
 > 相关链接：[高级路由配置文档](https://openrouter.ai/docs/guides/routing/provider-selection#advanced-sorting-with-partition)｜[Provider Explorer](https://openrouter.ai/providers)｜[Skills Loader 文档](https://openrouter.ai/docs/sdks/call-model/examples/skills-loader)  

##### **开源 RAG Demo Toolkit：用 OpenRouter 一键搭出品牌无关聊天助手**  
有开发者为面试写的 Agentic RAG Demo Toolkit 落选后选择开源：基于 OpenRouter API + FastAPI + Qdrant，可以把自己文档和 Logo 丢进去，就得到一套带检索、前端和演示视频的 RAG 机器人，非常适合初创或求职者拿去改改就用。  
 > 相关链接：[Agentic-RAG-Demo-Toolkit](https://github.com/chchchadzilla/Agentic-RAG-Demo-Toolkit)｜[演示视频](https://youtu.be/ZUTZMyKc5Bk)  

##### **Claude Code 技能系统和简化 Agent 开源，但被用户质疑成熟度**  
Anthropic 把 Claude Code 内部用于简化巨大 PR 的“代码简化 Agent”开源到官方插件仓库，并在 2.1 中加入递归 skill fork 等高级编排能力，支持多个子 Agent 各自有上下文。不过早期用户反馈不少：有时会删掉重要代码或做奇怪重构，说明这类“自动重构” Agent 还需要大量评测和保护措施。  
 > 相关链接：[code-simplifier 插件源码](https://github.com/anthropics/claude-plugins-official/tree/main/plugins/code-simplifier)｜[Reddit 讨论：效果和问题](https://www.reddit.com/r/ClaudeAI/comments/1q8h6oz/claude_code_creator_open_sources_the_internal/)｜[Claude Code 技能更新体验贴](https://www.reddit.com/r/ClaudeCode/comments/1q84z3u/the_skills_update_in_claude_21_are_just_amazing/)  

 

---  


#### **基础设施与硬件**  
##### **AI 计算量每 7 个月翻倍，单数据中心逼近 1GW 级别**  
Epoch AI 估算，全球 AI 计算总量按加速卡出货推断约 7 个月翻一倍，NVIDIA 占新增算力 60%以上。其分析还称 Anthropic 印第安纳州数据中心功率约 750MW，很快会到 1GW 级。算力和供电约束已经直接反向影响产品策略与“薅羊毛”管控。  
 > 相关链接：[全球算力增长分析](https://twitter.com/EpochAIResearch/status/2009757548891852929)｜[Anthropic 数据中心功率估算](https://twitter.com/EpochAIResearch/status/2009761084618797152)  

##### **Modal 分享 2 万块并发 GPU 运维经验，GPU 可靠性上升为一等公民**  
Modal 披露其在多云环境下同时管理 2 万+ GPU、累计 100 万+ 实例的经验，详细讲了公有云 GPU 在故障率、掉线和调度上的坑，并强调健康检查、多云冗余和调度策略是大规模推理/训练平台的必备能力。对自建或平台化团队很有参考价值。  
 > 相关链接：[Modal 工程实践推文](https://twitter.com/jonobelotti_IO/status/2009696881052729669)  

##### **torch 2.9 解决 flash_attn_varlen 与 compile 兼容问题，实测提速约 50%**  
GPU MODE 社区反馈，之前在 torch 2.4 用 flash_attn_varlen + torch.compile 时频繁 graph break，升级到 2.9 后问题消失，吞吐约提升 50%。nightly 版还加入更好的变长序列 API。很多人以为是“技术做不到”，结果只是版本太旧。  
 > 相关链接：[社区讨论记录](https://discord.com/channels/1189498204333543425/1189607750876008468/1459234506060861675)  

##### **ROCm 与 AMD 显卡体验：算力不差，生态和工具仍是主要门槛**  
多位用户分享 AMD 7900XTX 等卡在 Vulkan/ROCm 下的表现：纯算力接近 4090，但 CUDA 生态仍整体快约 10%，同时遇到 PyTorch 误识别 iGPU、ROCm 7.1 自动调优问题等。总体结论：预算有限、重视显存可以上 AMD，但要接受工具链坑多、调试成本高。  
 > 相关链接：[LM Studio 硬件讨论串](https://discord.com/channels/1110598183144399058/1153759714082033735/1458975310619082924)｜[ROCm 相关问题讨论](https://discord.com/channels/1189498204333543425/1233704710389764236/1459183441269489686)  

##### **消费级 GPU 选择：T4/A2000、7900XTX 与 RTX 5090 的取舍**  
社区给出的现实建议：75W 供电限制下可考虑 Tesla T4 / RTX A2000 做本地小模型；7900XTX 在价格/显存上很香但依赖 Vulkan/ROCm；最新 RTX 5090 被曝 VBIOS 最低功耗就 400W，不适合做“安静大显存工作站”。整体趋势是：算力不再稀缺，功耗和生态才是主要约束。  
 > 相关链接：[硬件选型讨论 1](https://discord.com/channels/1110598183144399058/1153759714082033735/1458975310619082924)｜[硬件选型讨论 2](https://discord.com/channels/1110598183144399058/1153759714082033735/1458975310619082924)  

 

---  


#### **研究与方法**  
##### **GDPO：针对多奖励的 RL 新算法，修补 GRPO 的“优势塌缩”问题**  
新工作 GDPO（Group reward–Decoupled Normalization Policy Optimization）提出对每个奖励单独归一化，避免 GRPO 中不同奖励组合被压成相同优势值的问题，从而在多目标 RL 下更稳定收敛。社区认为这解释了很多现有 GRPO 训练不稳定的现象。  
 > 相关链接：[GDPO 介绍线程](https://twitter.com/shizhediao/status/2009481573217784016)｜[对 GRPO 缺陷的评论](https://twitter.com/AliceInWeights/status/2009576516829774216)  

##### **LM Jigsaw：多模态大模型在 5×5 拼图上“突然掉崖”**  
新基准让 VLM 通过交换拼图块来还原图片：3×3 网格时前沿模型能做到约 95% 解出率，但到 5×5 时几乎 0%。同时 token 开销从 ~5.5 万涨到 ~34.5 万。作者认为现有模型更多是在做“补丁匹配”，而非真正的全局空间推理，对机器人、导航等场景是个警示。  
 > 相关链接：[项目主页与结果](https://filipbasara0.github.io/llm-jigsaw)｜[GitHub 仓库](https://github.com/filipbasara0/llm-jigsaw)｜[在线体验](https://llm-jigsaw.streamlit.app)  

##### **Eleuther 提出 CGGR 训练法：跳过部分梯度来省 75% 显存**  
Eleuther 社区有人提出 CGGR 训练方法，思路是在部分 step 跳过梯度，号称在 fineweb‑edu 上训练 SmolLM‑135M 时，batch=4 占 6–7GB，与普通训练 batch=1 相当，理论上可节省最多 75% 显存并加快训练。目前只是初步实验，大家在讨论用数学数据集做更系统的验证。  
 > 相关链接：[CGGR 初步讨论](https://discord.com/channels/729741769192767510/747850033994662000/1458944303085916233)  

##### **“死鲑鱼”论文再临：很多可解释性方法对随机网络也能编故事**  
Eleuther 有人分享《Dead Salmon: An Artifact of Random Initialization》预印本，指出特征归因、probe、稀疏自编码甚至因果分析，对随机初始化网络也能给出“看上去有道理”的解释。一位成员用这套思路给自己的 Fish Finder 工具做了 sanity check，确认结果里噪声很大，正在改进管线。  
 > 相关链接：[Dead Salmon 预印本](https://arxiv.org/abs/2512.18792)  

##### **DeepSeek MHC 等工作激发：大模型可扩展性更多要靠“结构约束”而非瞎堆宽度**  
围绕 DeepSeek MHC，Reddit 和 Discord 都在讨论：再往上堆深度/跨层连接，很容易信号爆炸/收敛失败，必须加几何或凸约束来“拴住”信息流。很多人把它类比为 ResNet 时代的小技巧——不是革命，但会变成大家默认会用的一类结构约束。  
 > 相关链接：[Reddit 机器学习讨论贴](https://www.reddit.com/r/MachineLearning/comments/1q893c1/d_deepseek_published_a_new_training_method_for/)  

 

---  


#### **产品与应用落地**  
##### **OpenAI 推出面向医疗的 ChatGPT 方案，多家大型医院已接入**  
OpenAI 宣布“OpenAI for Healthcare”，主打 HIPAA 合规和医疗场景定制，已在 AdventHealth、UCSF、Memorial Sloan Kettering 等机构上线。目标是把问诊记录整理、患者沟通、总结写文等标准化，医生端 AI 使用率据称一年内几乎翻倍。  
 > 相关链接：[官方介绍](https://openai.com/index/openai-for-healthcare/)  

##### **Claude Code 被开发者当“全栈搭档”：周末就做出多年不敢动的 POC**  
不少工程师在 Reddit 表示，自己原本只做基础设施或对底层/DB 不熟，借助 Claude Code 的多轮 refactor、瓶颈预判和代码自查，用一个周末就撸出以前认为“自己搞不定”的复杂项目。也有人提醒：AI 能加速实现，但仍会暴露架构本身不合理的问题，人类要负责“降复杂度”。  
 > 相关链接：[成功案例讨论](https://www.reddit.com/r/ClaudeCode/comments/1q8eik3/claude_code_has_allowed_me_to_execute_on_an_idea/)  

##### **LTX‑2、VeridisQuo、Synthia：开源生成媒体与检测工具齐上阵**  
一边是 LTX‑2 这类开源音视频生成模型压低门槛，另一边 HuggingFace 上的 VeridisQuo 深度伪造检测器用 GradCAM 热力图标出可疑区域；同时轻量合成数据工具 Synthia 在 1GB 显存上跑 LFM2.5‑1B 生成文本。生成、检测、合成数据链条基本配齐。  
 > 相关链接：[VeridisQuo 深度伪造检测](https://github.com/VeridisQuo-orga/VeridisQuo)｜[Synthia 演示视频](https://cdn.discordapp.com/attachments/897390720388825149/1458947573061783695/synthiashowcase1.mp4)｜[LTX‑2 模型](https://ltx.io/model)  

##### **Lovable：改一版系统提示，省下 2000 万美金云费用**  
AI 建站工具 Lovable 的工程师分享：通过系统提示优化（更少无用啰嗦、更明确任务边界），让模型推理速度提升约 4%，设计质量反而更好，按他们的调用量折算一年能省约 2000 万美元的 LLM 费用。说明“提示工程”对大规模产品是纯粹的成本问题，而不是玩文案。  
 > 相关链接：[成本优化经验贴](https://xcancel.com/benjaminvrbk/status/2009297105458716753?s=46)  

##### **本地 AI 实践：从多 GPU 桌面平台到低 VRAM 合成数据工具**  
Reddit 上有人花 9 个月做了本地多模态平台 Eloquent，支持多 GPU 切分、故事跟踪、人格评测等；社区也在讨论在无 GPU 的旧服务器上能否勉强跑小模型 RAG。综合建议：真正高并发还是要上显卡，小机器适合 7B 甚至更小、深度量化模型，RAG 要接受“记忆不完美”。  
 > 相关链接：[Eloquent 本地平台](https://github.com/boneylizard/Eloquent)｜[无 GPU 服务器跑 LLM 讨论](https://www.reddit.com/r/LocalLLM/comments/1q82yvp/llm_server_will_it_run_on_this/)  

 

---  


#### **行业与公司动态**  
##### **Anthropic 收紧 Claude Max 在第三方应用的使用，开发者被迫“去平台化”**  
多位开发者发现，Anthropic 限制用户在外部客户端里调用自己的 Claude 订阅，部分竞争产品甚至被直接掐断。这强化了一个现实：不能把关键工作流搭在单一厂商的消费者套餐上，大家开始默认做多模型封装、BYO‑key，并把“Max Plan” 当作随时会被收回的福利。  
 > 相关链接：[限制说明与开发者反馈](https://twitter.com/Yuchenj_UW/status/2009691122940211201)｜[相关讨论 1](https://twitter.com/andersonbcdefg/status/2009509161823031351)｜[相关讨论 2](https://twitter.com/gneubig/status/2009686033563316501)  

##### **MiniMax 在港 IPO，被包装为多模态“开放生态”**  
MiniMax 在港交所上市，媒体重点强调其早期押注统一多模态（文/语音/视频）模型和“开放生态”策略，配合面向开发者的编程计划争取第三方集成。IPO 带来的现金和员工套现，会让它在国内模型军备赛里更有子弹。  
 > 相关链接：[IPO 报道](https://twitter.com/business/status/2009478615453364599)｜[MiniMax 官方公告与生态宣传](https://twitter.com/MiniMax_AI/status/2009491818690547938)  

##### **中国 22 万亿美元居民储蓄被视为本土 AI 的潜在“弹药库”**  
分析指出，中国居民约有 22 万亿美元储蓄，历史上只有 5% 进入金融市场，如果再多投 5%，可释放 1 万亿美元增量资金。结合 Qwen、DeepSeek 等在开源圈的性价比优势，有观点认为中国厂商会走“能力 8 成、价格 1 成”的路线，用低价模型冲击欧美闭源订阅。  
 > 相关链接：[相关讨论贴](https://www.reddit.com/r/DeepSeek/comments/1q85fso/chinas_households_are_sitting_on_22_trillion_that/)  

##### **Protege AI 融资 3000 万美元，主打“为大模型补真数据”**  
数据公司 Protege AI 成立于 2024 年，宣布拿到 a16z 领投的 3000 万美元融资，要做跨行业、多模态的“真实世界数据”供应商，解决模型训练和对齐中的数据瓶颈。简单理解就是：把“数据标品化 + API 化”，卖给所有自己不想建数据团队的模型方/甲方。  
 > 相关链接：[融资公告](https://xcancel.com/withprotegeai/status/2009274652183363639?s=46)  

##### **OpenAI 转为盈利结构的诉讼获准进入陪审团审理**  
有关于 OpenAI 从非营利转向“封顶盈利”结构的诉讼被美国法官裁定可以进入陪审团审理阶段。结果未知，但意味着公司内部治理和对外叙事会被摆到台面上，证据和历史邮件可能被公开，对整个“大模型公司怎么设立”的法律先例影响不小。  
 > 相关链接：[案件报道](https://yro.slashdot.org/story/26/01/08/2230229/lawsuit-over-openai-for-profit-conversion-can-head-to-trial-us-judge-says)  

##### **LM Arena 数据：榜一模型平均只能稳 35 天**  
LMArena 披露统计：在其排行榜上，第一名模型平均只能维持约 35 天，5 个月后通常会跌出前五。也就是说“现在最强的模型”这个头衔本身非常短命，更有价值的是自动路由、快速评测和迁移能力，而不是死守某一个模型。  
 > 相关链接：[统计推文](https://twitter.com/arena/status/2009720083170636030)  

 

---  


#### **政策、治理与安全**  
##### **Radware 披露 ChatGPT 首个“零点击”服务端漏洞：发封邮件就能偷数据**  
Radware 研究中心称发现 ChatGPT 的服务端零点击漏洞：攻击者只需给用户发一封特制邮件，无需受害者点击，就可能让后端在处理邮件时泄露敏感数据。问题已披露给 OpenAI。对企业而言，这类“看似普通输入”的攻击面值得重点关注。  
 > 相关链接：[Radware 漏洞通告](https://www.radware.com/newsevents/pressreleases/2025/radware-uncovers-first-zero-click-service-side-vulnerability-in-chatgpt/)  

##### **OpenAI 与 Anthropic 的账单/配额争议：从“薅羊毛”到信任危机**  
有用户在 Reddit 投诉 OpenAI 多次把 20 美金 Plus“自动升级”成 200 美金 Pro，退款流程复杂；Claude Code Pro 用户则发现什么都没做用量就涨了几个点，怀疑客户端在后台疯狂发请求。社区建议用虚拟卡限额，同时也提醒：大厂订阅和使用统计黑盒太重，容易伤信任。  
 > 相关链接：[OpenAI 计费争议帖 1](https://www.reddit.com/r/OpenAI/comments/1q7yf8b/beware_of_openai_billing_practices/)｜[OpenAI 计费争议帖 2](https://www.reddit.com/r/ChatGPT/comments/1q7ym2a/beware_of_openai_billing_practices/)｜[Claude Code 用量异常讨论](https://www.reddit.com/r/ClaudeCode/comments/1q85sse/claude_code_pro_plan_hop_out_back_in_without_a/)  

##### **“无假冒法”与语音指纹：社区担心一刀切封死 TTS 创作**  
关于美国拟议的“NO FAKES Act”，有人指出其中的指纹识别条款可能要求所有语音合成都带“不可移除”的水印，等于从法律上锁死公开 voice cloning。反对者认为这会一路滑向禁止图像输入、视频生成，支持者则强调这是保护艺人和普通人不被随便合成发言。  
 > 相关链接：[相关 Reddit 讨论](https://www.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)  

##### **深度伪造风险上升：从“与聊天机器人相关死亡”词条到检测工具**  
有人建了“Deaths linked to chatbots”的维基词条，收集因聊天机器人错误或诱导导致的极端案例；同时 VeridisQuo 等开源检测器试图用可视化热力图标出伪造区域。讨论核心在于：责任在模型厂商、用户还是平台，以及监管要不要把“能生成就有锅”写进法律。  
 > 相关链接：[维基词条](https://en.wikipedia.org/wiki/Deaths_linked_to_chatbots)｜[VeridisQuo 检测工具](https://github.com/VeridisQuo-orga/VeridisQuo)  

##### **Anthropic 发布 Agent Evals 实战指南，强调“先从真实翻车案例做评测”**  
Anthropic 新博客系统梳理了 Agent 评测：区分能力评测和回归评测，引入代码/模型/人工 grader，以及 pass@k vs pass^k 等指标，重点建议从线上真实失败 trace 反推评测集。多位实践者认同：只有把指令、工具和评测一起迭代，才能避免“论文上很强，线上一直翻车”。  
 > 相关链接：[Demystifying evals for AI agents](https://twitter.com/AnthropicAI/status/2009696515061911674)  

 

---  

  

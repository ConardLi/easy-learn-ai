#### **Agent 与工具链**  
##### **Open Responses 统一 Responses API 规范，多家框架同步跟进**  
OpenAI 正式公开 Responses API 规范，并与 OpenRouter、Ollama、vLLM 等合作推出 Open Responses 开放规范，目标是统一各家模型的 JSON 接口、工具调用和流式行为，避免每换一家模型就要改一套 Agent 框架。目前 Anthropic、DeepMind 尚未加入。  
 > 相关链接：[Open Responses 官网](https://www.openresponses.org/)｜[OpenAI DevRel 公告](https://twitter.com/OpenAIDevs/status/2011862984595795974)｜[vLLM & Ollama 跟进讨论](https://twitter.com/reach_vb/status/2011863149356413275)｜[OpenRouter 宣布支持](https://twitter.com/OpenRouterAI/status/2011864089782599802)  

##### **Agent 架构共识：规划/执行/裁判分工 + 文件系统做记忆**  
Cursor 等一线产品总结，漫天“多智能体聊天”不好用，更稳定的是清晰角色分工：Planner 规划、Worker 执行、Judge 评审，并保持系统提示长期稳定。LangChain、LlamaIndex 等则统一指向一个趋势：用“虚拟文件系统”做上下文和记忆，再在其上挂技能和工具，背后通常是 Postgres 等数据库，而不是单纯磁盘。  
 > 相关链接：[Cursor 架构思路讨论](https://twitter.com/Yuchenj_UW/status/2011863636042469866)｜[Claude Code 子代理模式解析](https://twitter.com/omarsar0/status/2011823468468379782)｜[LlamaIndex 文件系统视角](https://twitter.com/jerryjliu0/status/2011849758944690625)｜[LangChain 文件系统 Agent 设计](https://twitter.com/LangChain/status/2011864707439690031)  

##### **LangChain JS 推“openwork” 桌面 Agent，强调真实进度反馈**  
LangChain JS 发布开源桌面 Agent（类似 Claude Code Cowork）：支持规划、子代理、文件系统记忆，npx 一条命令即可跑，兼容 Anthropic/OpenAI 模型。同时示范如何把工具调用事件流式推到 React 前端，避免 UI 只会“转圈圈”。  
 > 相关链接：[openwork 发布推文](https://twitter.com/LangChain_JS/status/2011863256223400360)｜[Agent 真实进度事件流示例](https://twitter.com/LangChain_JS/status/2011833970204557694)  

##### **MCP 社区：如何把服务器做成“无状态”，撑住多会话 Agent**  
Model Context Protocol 贡献者提出 signature 方法，让 MCP 服务器可以在保持 schema 固定的同时，根据会话动态暴露工具，从而用“无状态服务器 + 外部会话存储”支撑大量并发会话，避免现在一会话一进程的成本爆炸。  
 > 相关链接：[Signature 提案 PR #2091](https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2091)｜[动态工具集相关讨论](https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1442)  

##### **DSPy：自带工具 vs 模型原生工具，不能盲信，必须压测**  
DSPy 社区讨论指出，文档里说“DSPy 工具可能比模型原生工具调用更好”只是弱结论，实际完全取决于具体模型。即便同一厂商不同模型，工具调用质量差异也很大，最佳实践是：针对你自己的模型+程序组合做基准测试。  
 > 相关链接：[DSPy 工具调用文档](https://dspy.ai/learn/programming/tools/#using-native-tool-calling)  

 

---  


#### **模型与能力**  
##### **Black Forest Labs 发布 FLUX.2 Klein：4B Apache-2 开源、9B 基础大图模型**  
BFL 推出两款小型图像模型 FLUX.2 Klein：4B（Apache-2.0，可商用）和 9B（开放权重，偏研究/微调），主打 <1 秒快速生成和编辑。已上线 HuggingFace、fal、LMArena 等，被评价为“体积和效果都把早期 Stable Diffusion 拉开一个量级”。  
 > 相关链接：[官方发布](https://twitter.com/bfl_ml/status/2011825819082244266)｜[社区讨论与 Comfy 集成](https://www.reddit.com/r/StableDiffusion/comments/1qdmohb/flux2_klein_4b_9b_released/)｜[Arena 上线公告](https://twitter.com/arena/status/2011869067272208812)  

##### **Google DeepMind 推出 TranslateGemma：55 语种开源翻译模型族**  
DeepMind 基于 Gemma 3 和 Gemini 生成的翻译数据，发布 TranslateGemma 4B/12B/27B 多尺寸模型，覆盖 55 种语言，目标是低时延、可端侧部署的机器翻译。已有开发者在 iOS 上用 MLX+4B 量化跑起来。  
 > 相关链接：[官方介绍线程](https://twitter.com/GoogleDeepMind/status/2011848249850630363)｜[技术细节补充](https://twitter.com/GoogleDeepMind/status/2011848252451156244)  

##### **TII 发布 Falcon‑H1‑Tiny 系列：不到 1 亿参数面向端侧的专长小模型**  
阿联酋 TII 发布 Falcon‑H1‑Tiny 系列，多款 <100M 参数的专业小模型（代码、函数调用、多语种、推理等变体），定位是隐私友好的边缘/IoT 场景，而非云端通用大模型。  
 > 相关链接：[模型说明](https://twitter.com/yb2698/status/2011805117016916056)｜[TII 官方回顾](https://twitter.com/TIIuae/status/2012034581084430662)  

##### **StepFun Step‑Audio R1.1：32B 实时语音‑语音“推理”模型**  
StepFun 的 Step‑Audio R1.1（32B）在 Artificial Analysis 的 Big Bench Audio 跑到 96.4%，TTFT 约 1.51s，并给出了按小时和“等价 token”计价。定位是高质量实时语音对话/推理，而非简单 ASR+TTS 叠加。  
 > 相关链接：[评测与价格拆解](https://twitter.com/ArtificialAnlys/status/2012006066339581958)  

##### **Hawk Ultra 被吹成“Opus/Gemini 杀手”，单次生成 1.7 万行代码**  
LMArena 社区对 Movement Labs 的 Hawk Ultra 评价极高，有用户声称一次提示能吐出 1.7 万行代码，综合体验超过 Claude Opus 和 Gemini 3 Pro，并暗示后续会开源。目前缺乏系统 Bench，需要谨慎看待。  
 > 相关链接：[Movement Labs 宣传贴](https://x.com/movementlabsAI/status/2011964766533632380)  

 

---  


#### **基础设施与硬件**  
##### **NVIDIA 停产 RTX 5070 Ti、大幅砍 5060 Ti 16GB 供货，DIY AI 卡又涨价**  
多方消息称，因显存供应问题，NVIDIA 停产 RTX 5070 Ti，并大幅缩减 5060 Ti 16GB 产量，5070 Ti 已较 MSRP 涨价约 100 美元。5060 Ti 16GB 曾是性价比很高的 16GB CUDA 卡，适合本地 LLM 推理，玩家和“穷人 AI 机房”都受影响。  
 > 相关链接：[Reddit 讨论与视频源](https://www.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/)｜[视频来源](https://m.youtube.com/watch?v=yteN21aJEvE)  

##### **Together + Cursor 把 Blackwell 堆满：为“实时写代码 Agent”调教推理栈**  
Together 介绍其为 Cursor 提供推理服务的工程细节：在 GB200/B200 上用自研 Tensor Core 内核、FP4 量化、NVL72 mesh 并行等手段压低延迟，同时还要解决 NVLink 线缆更换等很“运维”的问题，才能撑住 IDE 级实时交互。  
 > 相关链接：[技术要点串讲](https://twitter.com/togethercompute/status/2011875191828488598)  

##### **Unsloth：RL 训练上下文拉到 700 万 token，vLLM 也能吃**  
Unsloth 发布长上下文 RL 方案，通过序列分块、隐藏态复用、log‑softmax 下 offload 等技巧，把 RL 训练支持的上下文拉到 700 万 token（号称较之前 7 倍），并与 vLLM 配合做推理优化，展示示例是单张 B200 上 38 万 token QLoRA。  
 > 相关链接：[Unsloth 公告](https://x.com/UnslothAI/status/2011827592886960131)｜[vLLM 合作确认](https://twitter.com/vllm_project/status/2011857612103630924)  

##### **GPU MODE 深挖 Hopper TMA/WGMMA：多维拷贝、swizzle 与性能坑**  
GPU MODE 社区详细讨论 Hopper 上 TMA tensor copy + WGMMA 的共享内存布局、LBO/SBO 设置及 2D vs 3D TMA 性能差异，给出可运行示例并提醒：有些场景多个 2D TMA 反而比单个 3D 更快，且 swizzle 会改变 LBO 行为。  
 > 相关链接：[示例代码 pipeline_tma_wgmma.cu](https://github.com/danielvegamyhre/gemm/blob/9fe95aa61ee7ebca4ded8b5029494b0d58e0d2e2/pipeline_tma_wgmma/pipeline_tma_wgmma.cu#L109-L118)｜[Colfax 教程参考](https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/)  

##### **Chrome Trace 在 600MB profile 上直接跪，Perfetto/ncompass 接盘**  
PyTorch Profiler 导出的 trace 文件接近 600–700MB 时，Chrome Trace 可视化经常空白或崩溃，实际远达不到文档标称的 1GB。社区推荐直接上 Perfetto UI，有人干脆做了 ncompass，把大 trace 切片后再看。  
 > 相关链接：[Perfetto 官方](https://perfetto.dev/)｜[ncompass 工具](https://docs.ncompass.tech)  

 

---  


#### **研究与方法**  
##### **“Focus” 记忆策略：让 Agent 自己决定何时总结、何时清理上下文**  
DAIR 推介一篇论文：给 Agent 增加 start_focus/complete_focus 两个控制点，Agent 觉得“该沉淀知识”时，把一段过程总结成一块长期知识，然后删掉过程 token。在 SWE‑bench Lite 上用 Claude Haiku 4.5 试验，token 开销减少约 22.7%，准确率不变。  
 > 相关链接：[DAIR 介绍帖](https://twitter.com/dair_ai/status/2011806092737827206)  

##### **评测圈再起争议：MMLU 数据泄露、答案格式“暗号”等问题被点名**  
研究者推出 MMLU‑Redux，手工清洗并重构泄露严重的 MMLU 子集；同时有人指出 MMLU‑Pro 部分化学/物理题存在“选项前空格就是正确答案”的伪特征。LMArena 也给出自己数据：整体上 OpenAI 领先，但专家级 Prompt 下 Anthropic 领先更频繁。  
 > 相关链接：[MMLU‑Redux 说明](https://twitter.com/PMinervini/status/2011782967723511868)｜[MMLU‑Pro 伪特征讨论](https://twitter.com/giffmana/status/2011859715043836166)｜[Arena 赛榜分析](https://twitter.com/arena/status/2011849440160858443)  

##### **全球 CoT 分析与“信息引力”：尝试用物理类比解释幻觉和不稳定**  
Eleuther 社区有人分享 LessWrong 的“Global CoT Analysis”，试图在大量链式思考样本上挖模型推理模式。GPU MODE 里则有人提出“Information Gravity”框架，用激发通量、阻尼等概念解释长对话中幻觉循环，并在 GitHub 放出实现。  
 > 相关链接：[Global CoT 分析文章](https://www.lesswrong.com/posts/q9g9zuudd3Pvw2cbj/global-cot-analysis-initial-attempts-to-uncover-patterns-1)｜[Information Gravity 仓库](https://github.com/brayo003/Substrate-X-Theory-of-Information-Gravity/tree/main)  

 

---  


#### **产品与应用落地**  
##### **VS Code 官网搜索重写：完全在浏览器里做向量检索**  
VS Code 团队重做官网文档搜索，开发 docfind，在浏览器内用 WebAssembly 跑检索，搜索速度明显提升，无需后端新服务。这类“前端自带小向量库”的模式，适合中小站点文档搜索。  
 > 相关链接：[VS Code 搜索改造介绍](https://twitter.com/code/status/2011827481175605487)  

##### **Qdrant × Tigris 推 RAG Lab：把“切块策略 A/B 测试”做成基础设施**  
Qdrant 与 Tigris Data 推出 RAG Lab，把“不同分段方式/索引配置下的检索效果对比”产品化：同一原始数据集 fork 成多份，每份配一套向量索引和参数，方便做可复现的 A/B 测试，而不是凭感觉改 prompt。  
 > 相关链接：[RAG Lab 介绍](https://twitter.com/qdrant_engine/status/2011679747244167175)  

##### **GitHub Copilot CLI/Agent 加自动记忆，开始长线“熟悉你的项目”**  
Copilot 的 CLI/编码 Agent 增加自动记忆功能，可以长期积累用户项目上下文，用于后续命令和建议。社区同时讨论一个“Copilot CLI SDK”，让开发者基于 Copilot 授权做自定义终端工具，比如一条命令生成视频脚本等。  
 > 相关链接：[自动记忆更新](https://twitter.com/_Evan_Boyle/status/2011932670096523326)｜[基于 Copilot CLI 的应用示例](https://twitter.com/burkeholland/status/2011934322413224152)  

##### **本地 LLM 推理 vs 云 API：Modal 实测成本和性能已经能打**  
Latent Space 讨论 Charles Frye 的一篇实战指南：在 Modal 上用本地部署 LLM，实测在单位成本+延迟上可以追平甚至超过主流 API，因此像会议记录转写、个人助手这类场景，完全可以考虑“本地/自托管 first”。  
 > 相关链接：[Modal 本地推理指南](https://xcancel.com/charles_irl/status/2011484220032762114?s=46)  

##### **“降噪阅读”Prompt 走红：不用“总结”，只删掉废话**  
有帖子分享一种替代“Summarize this”的用法：让模型做“噪声消除”，只高亮包含数据、时间、指令的句子，把形容词、故事性铺垫标记出来供忽略，文本长度可减约 70%，但不改写原文，减少幻觉风险。  
 > 相关链接：[Reddit 讨论贴](https://www.reddit.com/r/GeminiAI/comments/1qdfznb/we_stopped_using_summarize_this_we_reply_with_the/)  

 

---  


#### **行业与公司动态**  
##### **OpenAI 与 Cerebras 宣布 2028 年大规模合作，或对标 Groq 路线**  
OpenAI 公布与 Cerebras 的长期合作计划（目标时间点写到 2028 年），社区解读为在“非 NVIDIA 加速器”战线上补位，回应 Groq 等新硬件伙伴。Cerebras 早就能训 120B 级别模型，这次相当于被正式拉入一线供应商。  
 > 相关链接：[OpenAI 官方声明](https://openai.com/index/cerebras-partnership/)  

##### **Zhipu GLM‑Image：用华为昇腾 + MindSpore 训练，证明“不必依赖 NVIDIA + CUDA”**  
Zhipu 开源 GLM‑Image 图像模型，完全在华为 Ascend 910B + MindSpore 上训练。昇腾效率约为 NVIDIA 的 80%，但单卡更便宜、功耗更低。社区认为这给出了“开源阵营不靠 CUDA 也能卷起来”的样板，同时也抬高了对中国半导体（如 SMIC）的预期。  
 > 相关链接：[Reddit 讨论：不再需要 NVIDIA/CUDA？](https://www.reddit.com/r/DeepSeek/comments/1qdio2d/newly_released_glmimage_is_a_proof_of_concept/)  

##### **OpenAI 回聘三位老员工，其中包括 Thinking Machines 前 CTO/联合创始人**  
有帖子梳理，OpenAI 最近重新招回了 3 名前员工，其中包含曾在东南亚创业、任 Thinking Machines CTO 的研究者。社区感慨 AI 行业“人才旋转门”越来越快，也有人担心这会影响 Thinking Machines 传闻中的自研 LLM 进度。  
 > 相关链接：[Reddit 讨论](https://www.reddit.com/r/OpenAI/comments/1qdehxx/openai_rejoined_3_former_researchers_including_a/)  

##### **本地 LLM 爱好者把 32GB 企业卡价格炒翻：一帖发出，全网抢 w6800**  
有用户发帖分享 500 美元收的 w6800 32GB 大显存卡，结果被 /r/LocalLLaMA 社区抢购，价格迅速翻倍到 1000+，本人调侃自己制造了“小型淘金热”。评论区同时盘点 3090、R9700、MI50x 等“穷人工作站卡”的性价比。  
 > 相关链接：[Reddit 原帖](https://www.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)  

 

---  


#### **政策、治理与安全**  
##### **有用户称 GPT‑5.2 免费版“记忆”泄漏跨会话内容，引发隐私担忧**  
BASI Jailbreaking 里有人贴图称，开启 GPT‑5.2 免费账号的 memory 后，在新对话里看到了疑似其它会话内容，怀疑是状态隔离 bug。虽然目前只是单点案例，但在大家已在为 Agent/MCP 状态管理头痛的背景下，这类“记忆串线”非常敏感。  
 > 相关链接：[Discord 截图](https://cdn.discordapp.com/attachments/1228043845967544380/1461404780831445237/image.png)  

##### **Llama 3.2 安全策略升级，老越狱提示在新版本上失效**  
越狱社区反馈：在 Llama 3.1 上可用的一些典型 Jailbreak prompt，到了 Llama 3.2 就失效了，像制毒、极端减肥等敏感请求都被挡住。大家开始转向关闭“思考模式”、角色扮演等新技巧，同时推荐 Arcanum 的 AI 安全资源库系统化跟进攻防方法。  
 > 相关链接：[失败的旧越狱示例](https://chepenikconor.medium.com/day-855-9ae6f88b192c)｜[Arcanum AI 安全资源库](https://arcanum-sec.github.io/ai-sec-resources/)  

##### **社区涌现免费 AI 渗透测试资源，帮团队系统化做“AI 红队”**  
BASI 等安全社区在传播 Arcanum 的 AI 安全资源站：汇总越狱、提示注入、模型防御等教程，并给出一条“如何做 AI 渗透测试”的流程。很多团队打算按这个清单，给自家模型/产品做一轮系统性红队。  
 > 相关链接：[Arcanum 资源页](https://arcanum-sec.github.io/ai-sec-resources/)  

##### **Grok 应马斯克之令放宽图像审核，越狱社区准备冲击“史上最色情线程”**  
Elon Musk 在 X 上喊话让 Grok 放松图像内容审核，BASI 社区立刻组织人试图用色情图刷爆一条推文链，测试新策略底线。监管层面暂未有回应，但这类公开“拆安全阀”的行为，会给平台和模型厂商带来更多合规压力。  
 > 相关链接：[马斯克原推](https://x.com/elonmusk/status/2011527119097249996)  

##### **社区自建深度伪造检测认证课，招人试课共建标准**  
OpenAI Discord 有人准备做一个面向安全从业者和记者的“AI 深度伪造检测与认证”课程与考试，基于 PhantomTrace 平台。现在在招小规模试点用户，帮他们打磨实验和“及格标准”。这类民间认证，未来可能变成媒体/平台招聘时的参考。  
 > 相关链接：[项目招募说明](https://discord.com/channels/974519864045756446/1204360881593520128/1461532097641578672)  

 

---  

  

#### **模型与能力**  
##### **OpenAI 发布 GPT‑5.2‑Codex：长周期编码模型上线多家 IDE**  
OpenAI 将 GPT‑5.2‑Codex 上线到 Responses API，定位为当前最强“长任务”编码模型，可做大规模重构、找 bug，并被官方称为最擅长发现代码库安全漏洞的模型。Cursor、GitHub Copilot 等已第一时间集成，并开始在真实项目中跑长时间代理工作流。  
 > 相关链接：[OpenAI Dev 公告](https://twitter.com/OpenAIDevs/status/2011499597169115219)｜[Cursor 集成说明](https://twitter.com/cursor_ai/status/2011500027945033904)｜[GitHub @code 集成](https://twitter.com/code/status/2011503658815668623)  

##### **GPT‑5.2‑Codex 连续一周自主写浏览器：长时代理的实测案例**  
有团队在 Cursor 中用 GPT‑5.2 连续运行一周，生成 300 万行 Rust 代码，完成 HTML 解析、CSS 布局、绘制、JS VM 等，简单网页已能跑。这个案例成为“长时间连续代理”能力的标志，也让大家更重视在代理架构中加入强制人工 Review 环节。  
 > 相关链接：[作者线程](https://twitter.com/mntruell/status/2011562190286045552)｜[gdb 讨论](https://twitter.com/gdb/status/2011570314216718510)｜[关于审阅环路的讨论](https://twitter.com/scaling01/status/2011580895573262717)  

##### **GLM‑Image、LTX‑2、Veo 3.1：新一波多模态与视频模型**  
Zai 开源混合自回归+扩散图像模型 GLM‑Image，在文字渲染和知识型生成上表现突出；LTX‑2 作为开源视频模型，可生成最长 20 秒 4K 带声视频；Google Veo 3.1 增加竖屏、以图生视频和 1080p/4K 超分，已接入 Gemini、YouTube、AI Studio。  
 > 相关链接：[GLM‑Image 代码与介绍](https://github.com/zai-org/GLM-Image)｜[GLM‑Image 博文](https://z.ai/blog/glm-image)｜[LTX‑2 展示](https://x.com/venturetwins/status/2010878914273697956)｜[Veo 3.1 更新](https://x.com/tulseedoshi/status/2011174465720430612)  

##### **ERNIE‑5.0 进入 Text Arena 前十：首个上榜的中文大模型**  
ERNIE‑5.0‑0110 在 LMArena 文本榜单拿到总分 1460，排第 8，在专家模式排第 12，是首个进入 Top10 的中文模型，在数学和职业场景分项表现突出。  
 > 相关链接：[Text Arena 排行榜](https://lmarena.ai/leaderboard/text)｜[Leaderboard 变更日志](https://news.lmarena.ai/leaderboard-changelog/)  

##### **本地大模型实测：16GB 显存大约适合 14B 级模型**  
Reddit 多帖讨论 16GB 显卡能跑多大的本地模型：共识是 14B 左右较合适，既能留足上下文，又不用极端量化。30B 虽可通过深度量化和 CPU offload 勉强跑，但速度和质量都不理想，实际体验不如 14B。  
 > 相关链接：[16GB 显存可跑多大模型讨论](https://www.reddit.com/r/LocalLLM/comments/1qcuyh2/what_is_the_biggest_local_llm_that_can_fit_in/)  

##### **小设备本地跑 120B 模型的意义：便携隐私优先，而非性能**  
TiinyAI 推出一款 30W 功耗、80GB 内存、号称可本地跑 120B 模型的小主机。社区质疑其内存带宽和定价，但认为在断网、隐私要求极高或被审查环境下，本地超大模型设备有独特价值。  
 > 相关链接：[TiinyAI 设备讨论](https://www.reddit.com/r/LocalLLM/comments/1qcu498/small_ai_computer_runs_120b_models_locally_any/)  

##### **数学专项 Gemini 与 GPT‑5.2 在困难数学题上出现新成果**  
Google 的数学专项 Gemini 被称已证明一个新定理，另有工作用 5.2 Pro 在 Moser 虫子问题上刷新上界，并经 INRIA 数学家验证。实践表明，只要屏蔽网络、提供工具和文献、强制模型“死磕”，前沿难题也能取得实质进展。  
 > 相关链接：[Gemini 数学定理论文](https://arxiv.org/abs/2601.07222)｜[5.2 Pro 解决 Moser 问题推文](https://x.com/A_G_I_Joe/status/2011213692617285729)  

 

---  


#### **Agent 与工具链**  
##### **LangSmith Agent Builder 上线：官方给你一套“多代理工程脚手架”**  
LangChain 推出 LangSmith Agent Builder，用“文件系统视角”管理代理，内置记忆、触发器、技能/MCP/子代理等。官方也给出实践建议：绝大多数场景先用单代理，只有遇到上下文、所有权或分解瓶颈再拆多代理。  
 > 相关链接：[LangSmith Agent Builder 发布](https://twitter.com/LangChain/status/2011501888735494184)｜[多代理模式与最佳实践](https://twitter.com/LangChain/status/2011527733176856671)  

##### **“技能”成为通用插件层：Antigravity、MCP、CLI 正在收敛**  
Phil Schmid 为 antigravity 推出 Agent Skills 规范，用固定目录结构存放技能，在 Gemini CLI、Claude Code、OpenCode 等之间复用。HF 工程师认为，相比庞大插件生态，小而垂直的技能 + CLI/MCP 组合，更容易长期维护。  
 > 相关链接：[Agent Skills 介绍](https://twitter.com/_philschmid/status/2011345054343053370)  

##### **Claude Code、Cursor、Windsurf 等 IDE 争抢 GPT‑5.2‑Codex 长任务场景**  
Cursor 称 GPT‑5.2‑Codex 是“长时间任务前沿模型”，Windsurf 已内置并提供 0.5x–2x 不同推理强度价档；社区在测试其规划能力，有人吐槽 Codex 版在规划上甚至不如通用 GPT，需要更好的工作流和审阅机制兜底。  
 > 相关链接：[Windsurf 宣布支持 GPT‑5.2‑Codex](https://discord.com/channels/1027685395649015980/1027688115592237117/1461065370063339562)｜[Cursor 社区反馈](https://discord.com/channels/1074847526655643750/1074847527708393565/1460684446096166923)  

##### **Claude Code /compact 丢上下文之谜：社区给出本地文件+检索替代方案**  
用户发现 Claude Code 的 /compact 会在服务端只保留摘要，原文难以恢复，导致“记忆蒸发”。社区提出做法：先把长消息写入本地文件，压缩后只保留摘要+文件引用，再通过本地全文检索按需拉回细节，类似 Cursor 的动态上下文发现。  
 > 相关链接：[社区改造 /compact 讨论](https://www.reddit.com/r/ClaudeCode/comments/1qcjwou/figured_out_why_compact_loses_so_much_useful/)  

 

---  


#### **基础设施与硬件**  
##### **OpenAI 与 Cerebras 结盟：推理速度正式变成“产品特性”**  
OpenAI 宣布与 Cerebras 建立算力合作，被视为对 Groq 等专用推理硬件的回应。Cerebras 正在用自家芯片高吞吐服务 GLM‑4.7 等模型，业界普遍认为延迟和 tokens/s 已变成 ChatGPT 类产品的核心差异点，而不只是底层基础设施指标。  
 > 相关链接：[Cerebras 官方公告](https://twitter.com/cerebras/status/2011531740804964855)｜[OpenAI 合作说明](https://openai.com/index/cerebras-partnership/)  

##### **GLM‑4.7 多家服务商横评：Cerebras 吞吐最高，GPU 方案上下文更长**  
Artificial Analysis 对多家 GLM‑4.7 服务商做了对比：Cerebras 约 1445 token/s，TTFAT ~1.6s；Fireworks、Baseten 等 GPU 方案吞吐略低，但大多支持 200k 上下文（Cerebras 约 131k），可配合不同缓存和折扣策略。  
 > 相关链接：[GLM‑4.7 供应商评测](https://twitter.com/ArtificialAnlys/status/2011581689567592641)  

##### **Modal、自建推理与 2 万块 GPU 运维经验公开**  
Modal 发文称在很多场景自建推理已经能比公有 API 更便宜，并给出代码示例和经验。SemiAnalysis 同时解读了 Modal 如何运营 2 万块 GPU 集群，vLLM、FlashInfer 等配合大批量推理，把 H100 利用率“榨干”。  
 > 相关链接：[Modal 自建推理指南](https://twitter.com/charles_irl/status/2011484220032762114)｜[SemiAnalysis 解读 Modal 运维](https://twitter.com/SemiAnalysis_/status/2011498598043660777)｜[vLLM 批量推理实践](https://twitter.com/vllm_project/status/2011585247297880501)  

##### **Helion 0.2.10、FP8 Primer：训推一体栈继续往低精度走**  
NVIDIA 发布 TransformerEngine FP8 教程，社区讨论未来 NVFP4 训练格式；PyTorch Helion 0.2.10 支持 flex attention 示例 kernel 和 SM oversubscription，使持久内核在负载抖动时利用率更稳定。  
 > 相关链接：[TransformerEngine FP8 教程](https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/fp8_primer.ipynb)｜[Helion 0.2.10 发布](https://github.com/pytorch/helion)  

##### **NVLink 6 与多 GPU 一致性：实测和模型开发者的疑问**  
GPU MODE 社区在寻找 NVLink 6“72 块 GPU 像一块卡”这一宣传语背后的真实收益，希望看到跨 GPU 一致内存的基准数据。讨论集中在 B200 不稳定、NCCL 在多节点 8B 训练挂死等现实问题上，说明硬件规格到实际收益之间还有不少坑要填。  
 > 相关链接：[NVLink 讨论摘录](https://discord.com/channels/1189498204333543425/1189607726595194971/1460747616865226833)  

 

---  


#### **研究与方法**  
##### **DroPE、Engram、Ministral3：长上下文与“小大模型”的几条新路**  
Twitter 上有三条研究线被工程师讨论：DroPE 主张直接去掉 RoPE 再微调以改善长上下文；DeepSeek/PKU 的 Engram 模块用哈希 O(1) 稀疏记忆表把“记忆”从计算中分离；Mistral 的 Ministral3 报告系统总结了层剪枝、PCA 旋转、在线 DPO 等小模型瘦身配方。  
 > 相关链接：[DroPE 讨论](https://twitter.com/gabriberton/status/2011326182986564090)｜[Engram 介绍](https://twitter.com/LiorOnAI/status/2011468534887469448)｜[Ministral3 解读](https://twitter.com/eliebakouch/status/2011548952676499480)  

##### **多模态 RAG 新方案 UniversalRAG：先选模态再检索粒度**  
UniversalRAG 提出不要把所有模态硬塞进一个向量空间，而是先做模态路由，再在段落、整文、图像片段、视频片段等不同粒度间检索。路由可训练也可用大模型零样本决策，在 10 个多模态检索基准上都有明显提升。  
 > 相关链接：[UniversalRAG 介绍](https://twitter.com/omarsar0/status/2011442693134754243)｜[ViDoRe V3 基准](https://twitter.com/antonio_loison/status/2011398238910517249)  

##### **VLM 榜单并不稳：VPBench 展示“换个颜色就掉段位”**  
VPBench 发现，只是把图里的标记颜色从红换成蓝，就足以让很多视觉大模型在榜单上的名次大幅波动。对天天刷 leaderboard 的人是个提醒：细微呈现差异会放大成“性能差距”，结论要谨慎解读。  
 > 相关链接：[VPBench 论文讨论](https://twitter.com/lisabdunlap/status/2011521499182875116)  

##### **Spectral Sphere Optimizer：在“谱球面”上训练 LLM**  
新论文提出 SSO 优化器，对权重及更新施加谱约束，使最大奇异值受控，并与 muP 完全兼容，在 Megatron 框架下训练 1.7B 稠密和 8B MoE 模型优于 AdamW、Muon。实测显示激活更稳定、MoE 路由更均衡。  
 > 相关链接：[SSO 论文讨论串](https://www.reddit.com/r/MachineLearning/comments/1qcq27u/r_controlled_llm_training_on_spectral_sphere/)  

##### **SlopCodeBench：系统性暴露“写大项目的代理有多懒”**  
SprocketLab 发布 SlopCodeBench，设计多阶段大型编程任务，发现现有代码代理在早期架构决策和后续“整理重构”上表现很差，经常无法把临时实现抽象成可扩展方案。作者计划投稿 ICLR 工作坊，强调不应靠重度提示工程才能跑得像样。  
 > 相关链接：[SlopCodeBench 基准](https://github.com/SprocketLab/slop-code-bench)  

##### **上下文管理当作环境：长上下文性能可以“学会”管理**  
Unsloth 社区分享一篇论文，把上下文视为环境的一部分，让模型学会主动整理、删减、重排上下文，而不只是被动吃长序列。初步结果显示，这种“递归语言模型+代理壳”的思路，能在长上下文场景里显著减缓性能衰减。  
 > 相关链接：[上下文作为环境论文](https://arxiv.org/abs/2512.24601)  

 

---  


#### **产品与应用落地**  
##### **Google 推出 Universal Commerce Protocol：给电商代理的一套“标准动作”**  
Google 开源 Universal Commerce Protocol（UCP），让 AI 代理可以标准化地逛商品、加购物车、支付。协议内含 Agent2Agent 工作流、支付协议 AP2、以及与 vLLM/Ollama 等 LLM 栈对接的 MCP。社区关心的点是：有多少零售商会真正接入，以及 Google 会维护多久。  
 > 相关链接：[UCP 仓库](https://github.com/Universal-Commerce-Protocol/ucp)｜[Reddit 讨论](https://www.reddit.com/r/LocalLLM/comments/1qcpoaw/google_just_opensourced_universal_commerce/)  

##### **本地健康日志应用 Loggr：用 Apple Silicon+Qwen VLM 做手写 OCR**  
Loggr 在 Apple Silicon 上做离线健康日记，前端自研 NLP 管线，延迟低于 100ms；新增功能是用 Qwen2.5‑VL‑3B/7B（经 MLX 量化）识别手写日记，夜间批处理。社区建议尝试 PaddleOCR 或 MiMo‑VL‑7B‑RL 以提升脏手写效果。  
 > 相关链接：[Loggr 招募测试者](https://www.reddit.com/r/LocalLLaMA/comments/1qcd8sw/using_local_vlms_for_ocr_to_feed_into_an_nlp/)｜[MiMo‑VL‑7B‑RL 模型](https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-RL-2508)  

##### **AI 外联工作坊：Clay + LLM 的“批量私信”流水线教程**  
Chipro 社区将举办两场“外联提示工程”工作坊，讲如何用 Clay + AI 搭建端到端外联流水线：目标筛选、名单、富化、模板生成、A/B 测试，配合 Apollo、Attio、n8n 等。主办方宣称可以做到 40%+ 接受率和 18%+ 回复率。  
 > 相关链接：[工作坊报名页](https://luma.com/jt1vr0u5)  

##### **Manus × Similarweb 爆雷：用户几秒钟烧掉数千积分**  
Manus 新接入 Similarweb 后，有用户反馈单次运行几秒就消费 2500–5000 点数，完全没提示，客服响应又极慢，引发大量抱怨。大家呼吁增加消费上限、预估提示和“保险丝”，否则企业很难放心接入这类昂贵工具。  
 > 相关链接：[Manus 积分异常讨论](https://manus.im/share/wQ71wRcDWyDTQpH26bZP7v)  

##### **Code‑jp：面向本地模型的免费 IDE**  
有人在 HuggingFace 社区发布 Code‑jp，一款基于开源 VS Code 魔改的本地 AI IDE，支持 Ollama、LM Studio，后续会直接支持 llama.cpp。作者强调完全免费且不依赖 GitHub Copilot 之类闭源后端。  
 > 相关链接：[Code‑jp 官网](https://Code-jp.com)  

 

---  


#### **行业与公司动态**  
##### **Airbnb 挖走 Meta Llama 负责人当 CTO：开放模型成履历加分项**  
原 Meta Llama 负责人 Ahmad Al‑Dahle 宣布出任 Airbnb CTO，特别强调 Llama 开源后 1.2B+ 下载、6 万+ 衍生模型。HF CEO 等人解读为：中大型互联网公司也能在开源 AI 和开放科研上起到很大作用，不再只是大厂实验室的游戏。  
 > 相关链接：[Ahmad 加入 Airbnb 声明](https://twitter.com/Ahmad_Al_Dahle/status/2011440460821320056)｜[Clement Delangue 评论](https://twitter.com/ClementDelangue/status/2011477703698895245)  

##### **OpenAI/TML 人事洗牌：Soumith 当 CTO，Barret Zoph 回归 OpenAI**  
Mira Murati 宣布 Soumith Chintala 出任 Thinking Machines Lab CTO，Barret Zoph 离任不久后，OpenAI 又宣布 Zoph、Luke Metz、Sam Schoenholz 回归。高端研究人才在 OpenAI 内外轮换，加深外界对其组织架构和研究方向调整的猜测。  
 > 相关链接：[Mira 人事公告](https://twitter.com/miramurati/status/2011577319295692801)｜[Zoph 回归声明](https://twitter.com/barret_zoph/status/2011593621435531355)  

##### **Diffraqtion 融资 420 万美金：想用“量子光学镜头”重建视网膜**  
AI 视觉创业公司 Diffraqtion 完成 420 万美元 pre‑seed，ADIN 等参投。公司在做可编程量子光学器件，用特定波前形状直接在光学层实现“推理优化”，目标是辅助重建视网膜和更高质量视觉采集。  
 > 相关链接：[融资公告](https://x.com/adinonline/status/2011101500869623890)  

##### **OpenRouter 开源生态加速：awesome‑openrouter & apps 仓库上线**  
OpenRouter 团队新建 awesome‑openrouter 和 openrouter‑apps 仓库，鼓励社区提交集成案例（如 JanitorAI）和示例应用，希望把多模型、多供应商接入做成更统一的“基础设施层”。  
 > 相关链接：[awesome‑openrouter](https://github.com/OpenRouterTeam/awesome-openrouter)｜[openrouter‑apps](https://github.com/OpenRouterTeam/openrouter-apps)  

 

---  


#### **政策、治理与安全**  
##### **Chutes 采用 TEE 做“可验证隐私”推理，OpenRouter 生态需配合调整**  
推理服务商 Chutes 宣布全面迁移到 TEE（可信执行环境）架构，承诺对企业用户提供可验证的隐私保障。由于 TEE 对模型加载和网络有额外限制，OpenRouter 上部分模型（如 R1 0528）暂时下线，后续需按新架构恢复。  
 > 相关链接：[Chutes 架构说明](https://chutes.ai/news/confidential-compute-for-ai-inference-how-chutes-delivers-verifiable-privacy-with-trusted-execution-environments)  

##### **越狱社区新动向：Grok、Gemini、Llama 3.2 成重点“攻坚目标”**  
BASI 等越狱社区集中讨论如何绕过 Grok 图像安全、Gemini 3.0 Pro 限制和 Llama 3.2 新版安全策略，包括把 jailbreak 写进 Gemini 的个性化设置、利用哲学语录+黑话混淆等。Google AI Studio 被提醒会记录越狱数据用于训练，很多 payload 可能会快速失效。  
 > 相关链接：[BASI 越狱讨论示例](https://github.com/elder-plinius/L1B3RT4S)  

##### **关于 LLM 抽取和版权风险的担忧再次被提起**  
Eleuther 社区有人担心最近的“LLM 抽取分析”研究会被外界误读：论文展示模型会复现小说人物和情节，这在法律和舆论上可能被放大成“系统性抄袭”，而很多技术背景的细节不容易被非技术读者理解。  
 > 相关链接：[相关抽取论文](https://arxiv.org/abs/2402.14873)  

 

---  

  
